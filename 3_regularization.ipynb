{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning with the logistic regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0001\n",
      "0.0002\n",
      "0.0003\n",
      "0.0004\n",
      "0.0005\n",
      "0.0006\n",
      "0.0007\n",
      "0.0008\n",
      "0.0009\n",
      "0.001\n",
      "0.0011\n",
      "0.0012\n",
      "0.0013\n",
      "0.0014\n",
      "0.0015\n",
      "0.0016\n",
      "0.0017\n",
      "0.0018\n",
      "0.0019\n"
     ]
    }
   ],
   "source": [
    "test_prediction_accuracy = []\n",
    "reg_par = np.arange(0, 0.002, 0.0001)\n",
    "for i, beta in enumerate(reg_par):\n",
    "    \n",
    "  batch_size = 128\n",
    "  #beta = np.arange(1e-4, 1e-3, 1e-5).tolist()\n",
    "\n",
    "  graph = tf.Graph()\n",
    "  with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    reg = tf.nn.l2_loss(weights)\n",
    "    print(beta)\n",
    "    new_loss = loss + beta*reg\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "  num_steps = 3001\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    #print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      #if (step % 500 == 0):\n",
    "        #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        #print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          #valid_prediction.eval(), valid_labels))\n",
    "    #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    test_prediction_accuracy.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYVOWVx/HvAUQEFARFBQVxC+roKIRFSaQRRGKiLDFK\ndEyMiaCGRNGMJiYGjCFoJoSo0SQYJZJgxEFkiwuLtssoYFgEkU1BkVVUFpWt7T7zx3vRtqmmq7tv\n1a2u/n2ep56uunXvW6cuRZ1612vujoiISFl1kg5ARERykxKEiIikpAQhIiIpKUGIiEhKShAiIpKS\nEoSIiKSUVoIwsyFm9rqZLTKzcWZW38x6mNk8M1tgZi+Y2XEpjmtjZjvMbH50uz/+tyAiIplgFc2D\nMLOWwEtAO3ffY2bjgSeBW4EL3X2FmV0LdHT3q8oc2waY6u6nZyZ8ERHJlHpp7lcXaGRmJcBBwDqg\nBGgaPd8EWF/OsVatCEVEJBEVJgh3X29mI4E1wA5gurvPNLOrgSfNbAewHehSThHHmtm8aJ/b3P2l\nmGIXEZEMqrAPwsyaAn2ANkBLQk3icmAI0NvdWwNjgFEpDl8PtHb3DsBNwCNm1jiu4EVEJHPSaWLq\nCaxy9w8BzOwJoCtwurv/O9rnMeCpsge6exGwJbo/38zeAk4C5pfez8y0IJSISBW4e8aa8dMZxbQG\n6GJmDczMgB7AEqCJmZ0Y7dMLWFr2QDM7zMzqRPePA04AVqV6EXfXLabb0KFDE48hn246nzqfuXrL\ntHT6IOaa2QRgAVAU/R0NrAUeN7NiQi3hKgAzuxDo4O7DgHOAX5lZEVAMDHL3rZl4IyIiEq+0RjG5\n++3A7WU2T45uZfedCkyN7k8EJlYzRhERSYBmUuehgoKCpEPIKzqf8dL5rDkqnCiXlSDMPBfiEBGp\nScwMT7iTWkREaiElCBERSUkJQkREUlKCEBGRlJQgREQkJSUIERFJSQlCRERSUoIQEZGUlCBERCQl\nJQgREUlJCUJERFJSghARkZSUIEREJCUlCBERSUkJQkREUlKCEBGRlJQgREQkJSUIERFJSQlCRERS\nUoIQEZGUlCBERCQlJQgREUlJCUJERFJSghARkZSUIEREJCUlCBERSSmtBGFmQ8zsdTNbZGbjzKy+\nmfUws3lmtsDMXjCz48o59mdmttLMlppZr3jDF8msefNg3bqkoxBJhrn7/ncwawm8BLRz9z1mNh54\nErgVuNDdV5jZtUBHd7+qzLEnA48AHYGjgZnAiV7mRc2s7CaRxO3cCSecAPXqwfTp8KUvJR2RyBeZ\nGe5umSo/3SamukAjM6sHHASsA0qAptHzTYD1KY7rAzzq7p+6+9vASqBTtSIWyZI//xk6dYKhQ6Gg\nABYsSDoikeyqV9EO7r7ezEYCa4AdwHR3n2lmVwNPmtkOYDvQJcXhrYBXSj1eF20TyWmffAK//S08\n8wycfjo0bQrnnw+PPw5f/WrS0YlkR4U1CDNrSqgJtAFaEmoSlwNDgN7u3hoYA4xKdXiKbWpLkpz3\nxz/COeeE5ADQvz+MGxf+PvlksrGJZEuFNQigJ7DK3T8EMLMngK7A6e7+72ifx4CnUhy7Fjim1OOj\nSd0UxbBhwz67X1BQQEFBQRqhicRv+3YYORKef/6L2887D6ZMgb594Z574NJLk4lPaq/CwkIKCwuz\n9nrpdFJ3Ah4kdDTvJtQWXgWGAl3dfaWZfZ9Qm/hWmWNPAcYBnQlNSzNQJ7XkuF//GpYtg3/8I/Xz\nixdD796hb2LgwOzGJlJapjup0+mDmGtmE4AFQFH0dzShdvC4mRUDW4CrooAvBDq4+zB3f8PMHgPe\niI69TplActnWrXD33fDyy+Xvc9ppoXbRqxds2QK33JK9+ESyqcIaRFaCUA1CcsQvfwlr18JDD1W8\n77p1IUlceCGMGAGWsd9xuWHtWjjiCDjggKQjkb1yZZirSN774AO4/3647bb09m/VCl54AZ59Fq69\nFoqLMxtfEnbtgkcegW7dwpyQK64A/ZarPZQgRCL/8z9w8cXQtm36xzRvDrNmwYoVcPnlsGdP5uLL\npmXL4Kab4Jhj4G9/gx//GDZvhrffhjvuSDo6yRY1MYkAmzbBKafAwoXhS7Gydu0Ko5qKimDCBGjY\nMP4YM23XLpg4Ef7yF1i+HL73Pbj6ajiu1CI6GzeGyYMjR8K3vlV+WZIdmW5iUoIQAW68ET79NAxf\nrapPP4WrroLVq2HaNGjSJL74MmnZMhg9Gv7+dzjzTBg0KPSr1K+fev8FC0Lfy9NPQ4cO2Y1VvkgJ\nQiTD1q8PI5Nefx2OOqp6ZZWUwPXXw0svhVnYLVrEE2Pcdu0Ks8JHjw7NY9/7HvzgB1+sLezPxInh\nfc6ZAy1bZjZWKZ8ShEiGDR4MDRrA734XT3nuMGwYPPoozJgBrVvHU24cStcW2rcP8zguuqhqI5OG\nD4fJk8OQ34MOij9WqZgShEgGrVkTmlWWLYPDD4+37D/8AUaNyo2VYJ98Eu66q2q1hfK4w3/9Vxi9\n9c9/5v8w31ykBCGSQQMHwmGHwW9+k5nyx4yBW28N5V9xRVg6PJuWLYMhQ0K/yK9+Bf36xTuPYedO\n6N4dLrggzCGR7FKCEMmQVaugY8fwq7p588y9zuzZcPPN8P77IVH06ZP5X9tbt8Ltt4flQm69FX74\nw/I7natLI5uSo4lyIhlyxx2h/yGTyQGgS5fQTv+734X1m84+e9+FAONSXByGqbZrBzt2wJIloQaR\nqeQAcOSRoS/iuuvCFfgkf6gGIbXSihXQtSusXBmu9ZAtJSWhvf6220K/xIgRcMYZ8ZRdWAg33BCG\n1959d3zlpksjm7JPNQiRDLj99vBlms3kAFCnTphxvWwZfP3r8LWvwWWXwVtvVb3M1avDDPArr4Sf\n/zwkimwnBwjXyrjmmrAc+s6d2X99iZ8ShNQ6S5bAzJlh+Yik1K8fmrdWrgzNQZ06hccbN6Zfxscf\nwy9+AV/+Mvznf8LSpaEPIMnRRLfeCieeGEZK5WKjwM6d8MADocN++nTYti3piHKbEoTUOsOGwU9+\nAgcfnHQk0LhxGP2zbFkYXXTqqaH5aX9fXCUlofO5XbtQe1i4MByTC3MRzOCvfw1x5dKaTRs3hnPU\npg1MnRouKTt8eFhw8dRTw7DfBx+EN94I51cC9UFIrbJwYRiS+eabuble0jvvhI7sJ5+En/40dPw2\naPD583Pnhnb+oqLQz9C1a3Kx7s+GDdC5c/Ijm157LcxFmTw5NOVdfz2cdNLnzxcVhQtAvfLK57cP\nPwyxn3VWuHXqlP2myHRpmKtIjPr0gXPPDV8Uuez110N/woIFob+kZ8/wC3j69PDL97vfDf0ZuSyp\nNZtKSsJr/v73odlt8OCwvlSzZukdv2lTGJo8e3ZIGPPmhdnwexPGWWeF2lsunH8lCJGYvPpq6Ehd\nufKLv8pz2csvh5rE3LmhU/3WW+GQQ5KOKn3ZHNm0Y0dYQmTUqFA7vPFGuOSS6g/x/fRTWLTo8xrG\n7Nnh2iH9+oWJlp07J9fvowQhEpOvfS2sO3TttUlHUjnu4cuvUaOkI6maTK/ZtGED3HdfWGPqrLPC\nvI9u3TL7pb1hQ0hGo0eHf5eBA8OyI9lewVcJQiQGL78c2qBXrMjspDHZV6bWbKqofyEbSkrguedC\nopg+PdQqBg0K/RbZqFUoQYjEoEcP+Pa3w2gVyb6dO6GgAM47D77zneqVtWJFWAhx6VL40Y/Cr/d0\n+xcy6b33wtX3slmrUIIQqabCwpAYli6Nd6E6qZwNG+Cb3wxrUlXH4YeHZsI4+hcyoWyton//kCwy\nUatQghCpBnc455xw6czq/nIVqayytYpBg8JM+rhqFUoQItUwY0ZohliyBOrWTToaqa0yVatQghCp\nIvcwquWGG2DAgKSjEQn21ioeewxeeKF6EzaVIEQqaceOcE3oSZPgxRfDaJdcmNQkErdMJ4gsX99K\nJH7FxTB/fliAb8aMMKnszDPD7ONJk5QcRKpKNQipcdzD8th7E8Jzz4VZuj17hlu3brmxEJ9IpqmJ\nSQTYvBmefTYkhJkzwyJrPXuGcfU9esBRRyUdoUj2KUFIjbN9e+gc3rAhDO1r3Dj8LX0/nW27d4c5\nDDNmhOtHd+sWEkLPnmGxtCSveyCSC3IiQZjZEOD7QAmwGLgKmAE0BgxoAcxx9/4pji0GXov2e8fd\n+6bYRwkij1x3HezZA3fdFdbd//jj8Lf0/bJ/U22rUyfMYejZEzp21CQ3kbIS76Q2s5bAj4B27r7H\nzMYDl7r7OaX2mQBMKqeIT9y9fSzRSs578UWYMiUsV920KTRvnnREIlJV6Y7vqAs0MrN6QENg/d4n\nzOxg4FzKTxBqCKgldu0KM5bvvTd3L7AiIumrMEG4+3pgJLAGWAdsdfeZpXbpC8x094/LKeJAM5tr\nZi+bWZ9qRyw569e/Dpdv7Ncv6UhEJA7pNDE1BfoAbYBtwAQzu8zdH4l2+TbwwH6KaO3uG82sLfCs\nmS1y99Vldxo2bNhn9wsKCigoKEj7TUjyFi0Kywi89lrSkYjkr8LCQgoLC7P2ehV2UpvZxcD57n51\n9PgKoLO7DzazZsByoJW776nwxczGAFPdfWKZ7eqkrsGKi8OopYEDtZy2SDZlupM6nT6INUAXM2tg\nZgb0AJZGz10CTCsvOZhZUzOrH90/DDgbeKP6YUsuueeeMCz1+99POhIRiVOFTUzuPjcapbQAKIr+\njo6evgS4s/T+ZtYBGOTuA4GTgb9EQ13rACPcfVmM8UvCVq8Ol5ScPVvzEkTyjSbKSZW5Q69eYfLa\nzTcnHY1I7ZMLTUwiKY0dCx98ADfemHQkIpIJqkFIlWzaBKefDk8/HVZOFZHsy4mlNjJNCaLmGTAA\njj0W7ryzwl1FJEMSX2pDpKypU2HePBgzJulIRCSTlCCkUrZvD4vxjR0LBx2UdDQikklqYpJKue66\ncC2GB/Y3d15EskJNTJIzXnwRJk+GJUuSjkREskHDXCUtWqlVpPZRgpC07F2ptf8+l4QSkXylPgip\n0KJF4brPr70GLVsmHY2I7KWZ1JKo4uKwQuuIEUoOIrWNEoTs1z33QKNGWqlVpDZSE5OUa/Vq6NgR\nXnkFTjwx6WhEpCw1MUki3MMFgP77v5UcRGorJQhJae9KrTfdlHQkIpIUNTHJPvau1PrUU9C+fdLR\niEh5NJNasmLbNnj+eZg5E6ZNg6uuUnIQqe1Ug6il9uyBOXNCQpgxAxYvhi5dwtXhevYM13jQJURF\ncpuuByGxcA9rKO1NCC++CCed9HlCOPtsrc4qUtMoQUiVrV0Ls2aFhDBrFjRs+HlC6N4dmjdPOkIR\nqQ4lCKmU9evDVd5mzIDNm+Hcc0NS6NEDjjsu6ehEJE7qpJZKueUWqFMHxo2DM84I90VEqkI1iDzy\n5puho/mtt6BJk6SjEZFM00xqSduIEfDDHyo5iEg8VIPIE2+/DR06wMqV0KxZ0tGISDaoBiFpufPO\nsHaSkoOIxEU1iDywdm1YGmP5cjj88KSjEZFsUQ1CKvTb34alMZQcRCROadUgzGwI8H2gBFgMXAXM\nABoDBrQA5rj7PlcsNrPvAj8HHBju7mNT7KMaRBVt2BCuFf3GG3DkkUlHIyLZlPhEOTNrCbwEtHP3\nPWY2HvhX6S96M5sATHL3f5Q59lDg30B7QiKZB7R3921l9lOCqKKbboKionDlNxGpXXJlolxdoJGZ\nlQANgfV7nzCzg4FzgStTHHc+MH1vQjCz6UBvYHw1YpbI5s0wZgwsWpR0JCKSjyrsg3D39cBIYA2w\nDtjq7jNL7dIXmOnuH6c4vBXwbqnH66JtEoPf/x4uvRSOPjrpSEQkH1VYgzCzpkAfoA2wDZhgZpe5\n+yPRLt8GHijv8BTbUrYlDRs27LP7BQUFFBQUVBRarfbhhzB6NMyfn3QkIpIthYWFFBYWZu310umD\nuBg4392vjh5fAXR298Fm1gxYDrRy9z0pjh0AFLj7NdHjPwPPufv4MvupD6KShg4Nw1sffDDpSEQk\nKbnQB7EG6GJmDYDdQA/g1ei5S4BpqZJD5BlguJk1ITRnnQf8tHohy7ZtcN99MHt20pGISD5Lpw9i\nLjABWAC8Rmg2Gh09fQnwz9L7m1kHMxsdHbsFuIMwkmkOcLu7b40t+lrq3nvhggvghBOSjkRE8plm\nUtcwH30Exx8PL7wA7dolHY2IJEkzqeUL/vSncBEgJQcRyTTVIGqQTz4JtYcZM+C005KORkSSphqE\nfGb0aOjaVclBRLJDNYgYzZkTvrwbNoy/7F27Qu1h2jQ488z4yxeRmkc1iBpi82YoKAi3DRviL//B\nB6F9eyUHEcke1SBicscdsHo1HHdcaAqaMgXOOCOesnfvhhNPhAkToFOneMoUkZovFybKSQV274b7\n74fp00MT00knwXnnhV/9F11U/fIffhhOPlnJQUSySwkiBo8+Cv/xH593Hl9yCRx7LPTrF64RfeON\nYFXM8UVFMGIE/OMfFe8rIhIn9UFUkzuMGgVDhnxxe6dO8MorMHYsDBoUvuirYtw4aNs2jF4SEckm\nJYhqev75MMKod+99n2vdGl56KXRa9+4NW7ZUruziYvjNb+C22+KJVUSkMpQgqmnUKLjhBqhTzpk8\n+GCYNCl0WHfpEpqc0jV+PLRoEUZGiYhkm0YxVcObb8JZZ8E776Q392H06FAbGD++4i/9kpLQr/GH\nP0CvXrGEKyJ5RvMgctjdd8PVV6c/MW7gQHjkkXAVuIce2v++jz8eah/nnVf9OEVEqkI1iCraujXM\neVi8GFpV8iKqy5fDN74RRjndeee+zVMlJWFC3PDhYT8RkVRUg8hRDzwQrslQ2eQA8KUvhYv9zJ0L\n3/xmWISvtKlToW5d+PrX44lVRKQqlCCq4NNPw0V7yg5trYzmzcPEukMPha9+NVw+FMKw2TvugF/8\noupzJ0RE4qAEUQWPPx4mwnXoUL1y6tcPs60HDAid3fPmwVNPhZnZffvGEqqISJVpJnUVjBoFt9wS\nT1lmcPPNYa2l3r2hSZPQ91DesFkRkWzR11AlvfIKvPdePGssldavHzzzDHTrBhdfHG/ZIiJVoVFM\nlXTJJXD22WFynIhIkjI9ikkJohLeeScMP337bTjkkKSjEZHaTsNcc8i998KVVyo5iEjtoBpEmj76\nKIxcmjcv/BURSZpqEDnib3+D7t2VHESk9lANIg3FxWH288MP67oMIpI7VIPIAdOmQbNmYfSSiEht\noQSRhr1XjNPSFyJSmyhBVGDBAnjrLU1eE5HaJ60EYWZDzOx1M1tkZuPMrH60fbiZLTezJWY2uJxj\ni81svpktMLNJcQafDaNGweDBcMABSUciIpJdFXZSm1lL4CWgnbvvMbPxwL8IyaXA3a+M9jvM3d9P\ncfx2d9/vzIFc7aTesAFOOQVWrQqrroqI5JJMd1Knu1hfXaCRmZUADYH1wHDg23t3SJUcIjW25f6+\n++Cyy5QcRKR2qrCJyd3XAyOBNcA6YKu7zwSOBwaY2atm9i8zO6GcIg40s7lm9rKZ9Ykt8gzbuTNc\nQ/r665OOREQkGRXWIMysKdAHaANsA/7XzC4HDgR2uHtHM+sHPASck6KI1u6+0czaAs+a2SJ3X112\np2HDhn12v6CggIKCgiq8nfj8/e/QuTOcdFKiYYiIfKawsJDCwsKsvV46fRAXA+e7+9XR4yuALkB3\noLe7r4m2b3X3phWUNQaY6u4Ty2zPqT4Idzj1VPjjH+Hcc5OORkQktVyYKLcG6GJmDczMgB7AG8Ck\n6D5mVgAsL3ugmTUtNeLpMODs6Nic9swzYdRS9+5JRyIikpwKm5jcfa6ZTQAWAEXR39GEzupxZjYE\n+Aj4AYCZdQAGuftA4GTgL2ZWTEhGI9x9WUbeSYw0MU5ERGsx7WPJEujZM1zz4cADk45GRKR8udDE\nVKvcfTdce62Sg4iIahClvP8+nHgiLF8OLVokHY2IyP6pBpFFf/4z9O+v5CAiAqpBfGb3bmjbNoxg\nOu20REMREUmLahBZMn58mPug5CAiEihBRMaOhWuuSToKEZHcoSYm4MMPw7WmN2yARo0SC0NEpFLU\nxJQF06ZBjx5KDiIipSlBAE88Af36JR2FiEhuqfVNTJ98Ai1bwurV0KxZIiGIiFSJmpgy7JlnoGNH\nJQcRkbJqfYJQ85KISGq1uompqAiOOAIWL4ZWrbL+8iIi1aImpgwqLAxrLyk5iIjsq1YnCDUviYiU\nr9Y2MZWUwNFHh1qErjstIjWRmpgyZO5cOPRQJQcRkfLU2gSh5iURkf2rlQnCHSZOVIIQEdmfWpkg\nliyBPXugffukIxERyV21MkE88QT07QuWsa4dEZGar9YmCDUviYjsX61LEG+/De++C1/5StKRiIjk\ntlqXICZNgosugnr1ko5ERCS31boEoeYlEZH01KqZ1O+9FybGbdwIDRpk/OVERDJKM6ljNGUK9Oql\n5CAiko5alSDUvCQikr60EoSZDTGz181skZmNM7P60fbhZrbczJaY2eByjv2uma2I9vtOnMFXxvbt\n8OKLcMEFSUUgIlKzVDiWx8xaAj8C2rn7HjMbDwwwszpAK3f/UrTfYSmOPRT4JdAeMGCemU12921x\nvol0PPUUdO0KTZpk+5VFRGqmdJuY6gKNzKwe0BBYD1wL/GrvDu7+forjzgemu/s2d98KTAd6Vy/k\nqlHzkohI5VSYINx9PTASWAOsA7a6+0zgeEJN4lUz+5eZnZDi8FbAu6Uer4u2ZdWuXfD009CnT7Zf\nWUSk5qowQZhZU6AP0AZoSahJXA4cCOxw947AX4GHUh2eYlvWx9XOmgWnnRauPy0iIulJZz5xT2CV\nu38IYGZPAGcTagYTAdz9CTMbk+LYtUBBqcdHA8+lepFhw4Z9dr+goICCgoJUu1WJmpdEJB8UFhZS\nWFiYtdercKKcmXUCHgQ6AruBMcCrhKaile4+xswKgLvcvXOZYw8F/k3opK4T3e8Q9UeU3i9jE+WK\ni+Goo2DOHGjbNiMvISKSiExPlKuwBuHuc81sArAAKIr+jiZ0Vo8zsyHAR8APooA7AIPcfaC7bzGz\nOwiJwYHbyyaHTPu//4OWLZUcREQqK++X2hgyBJo2haFDM1K8iEhiEq9B1GTuof9hypSkIxERqXny\neqmNhQuhbt0wgklERConrxPE3tFLurSoiEjl5XWCmDgR+vdPOgoRkZopbxPEypXwwQfQpUvSkYiI\n1Ex5myCeeCIsrVEnb9+hiEhm5e3Xp2ZPi4hUT17Og1i/Hk49FTZtgvr1YytWRCSn6JKjVTB5crgw\nkJKDiEjV5WWCUPOSiEj15V0T05Yt0KZNaGZq3DiWIkVEcpKamCpp2jTo3l3JQUSkuvIuQah5SUQk\nHnnVxLRjR7j2w6pV0Lx5DIGJiOQwNTFVwvTp0KGDkoOISBzyKkGoeUlEJD5508RUVARHHhmW+D7m\nmJgCExHJYWpiStPzz8Nxxyk5iIjEJW8ShJqXRETilRdNTCUloeYwaxa0axdjYCIiOUzXpC6lqAje\ney8swrf376ZN8NZbcMghSg4iInHKmQQxZ87nX/ilv/xLb9u+HQ47DFq0gCOO+Px2/PFwzTVJvwMR\nkfySM01MX/6yf/aFXzYB7H3cvLkuACQislemm5hyJkHkQhwiIjWJhrmKiEgilCBERCQlJQgREUlJ\nCUJERFJKK0GY2RAze93MFpnZODM70MzGmNkqM1tgZvPN7PRyji2Onl9gZpPiDV9ERDKlwgRhZi2B\nHwHt3f10wtyJAYADP3H3M929vbsvKqeIT6Lnz3T3vrFFLuUqLCxMOoS8ovMZL53PmiPdJqa6QCMz\nqwc0BNYBFt0qkrEhWJKa/gPGS+czXjqfNUeFCcLd1wMjgTWExLDV3WdGT//azBaa2UgzO6CcIg40\ns7lm9rKZ9YknbBERybR0mpiaAn2ANkBLoLGZXQb81N1PBjoCzYFbyimitbt3Ai4H/mBmbWOJXERE\nMqrCmdRmdjFwvrtfHT2+Aujs7oNL7dMNuMndL6qgrDHAVHefWGa7plGLiFRB0qu5rgG6mFkDYDfQ\nA3jVzI50941mZkBf4PWyB0a1jx3uvsfMDgPOBu4qu18m36CIiFRNhQnC3eea2QRgAVAEzAdGA09H\nX/oGLASuATCzDsAgdx8InAz8xcyKCc1ZI9x9WUbeiYiIxConFusTEZHcE9tMajPrbWbLzGyFme3T\nYW1m9c3sUTNbaWavmFnrUs/9LNq+1Mx6VVSmmR1rZrPNbLmZ/TMafps3snwu05rwWJNl6Hw+aGab\nzGxRmbIONbPp0WfzGTNrktl3l31ZPp9DzWxt9Nmcb2a9M/vusi/u82lmR5vZs2b2hpktNrMfl9q/\ncp9Pd6/2jZBo3iSMdDqA0OTUrsw+1wL3R/cvBR6N7p9CaL6qBxwblWP7KxMYD3wruv8nQpNWLO8l\n6VsC53IM0C/p912Tzmf03FeAM4BFZcq6C7g5un8LcGfS56CGn8+hwI1Jv++adD6BI4Ezon0aA8tL\n/X+v1OczrhpEJ2Clu7/j7kXAo4ShsaX1AR6O7k8Azo3uXxS94U/d/W1gZVTe/so8F3g8uv8w0C+m\n95ELsn0uIb/X5MrE+cTdXwK2pHi90mU9TBjAkU+yfT4hvyfbxn4+3X2juy8EcPePgaVAqxRlVfj5\njOuLoRXwbqnHa0sFtM8+7l4MbDOzZimOXRdtS1mmmTUHtrh7SantLWN6H7kga+ey1ON0JjzWVJk4\nn/vTwt03RWVtBA6veug5KdvnE+CH0efzr3nYZJfR82lmxxJqZrOjTZX6fMaVIFJl+LK93+XtU5Xt\nZZ/Lp572bJ5LSH/CY02VifNZm2X7fN4PHO/uZwAbgd9XGGHNkrHzaWaNCTWO6939k6oEF1eCWAu0\nLvX4aGB9mX3eBY4BMLO6QBN33xIde0yKY1OW6e7vA03NrE6Z/fNF1s4lQKlfE0WE/ohOcb2RHJGJ\n87k/m8zsiKisI4H3qh56Tsrq+XT3zR41mAMPEH7I5JOMnM9o4M4E4O/uPrnUPpX6fMaVIF4FTjCz\nNmZWn7Ai4wvpAAABHUlEQVTa65Qy+0wFvhvd/xbwbHR/CjAg6qlvC5wAzC2nzL1v9NmoDKIyS5+A\nmi5b53IKfPYhwaz8CY81XCbO516parNTgCuj+/n22YQsn8+9n89If/T5TPd8PgS84e53lymrcp/P\nGHvjexN6y1cSmi0Abge+Ed0/EHgsen42cGypY39G6IFfCvTaX5nR9rbAHGAFYUTTAUmPRojzluVz\nOQt4DVgEjAUaJv3+a8j5fITwa203YbWB70XbmwEzo9ebATRN+v3X8PM5NvpsLgQmAUck/f5z/XwC\nXYHi6JwtIExu7l2Vz6cmyomISEr5PLxRRESqQQlCRERSUoIQEZGUlCBERCQlJQgREUlJCUJERFJS\nghARkZSUIEREJKX/B9UMvKXYgs0lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48f93db090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(reg_par,test_prediction_accuracy)\n",
    "print(max(test_prediction_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.352833\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 500: 1.293468\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 1000: 0.855478\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1500: 0.813192\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 2000: 0.774765\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2500: 0.475428\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 3000: 0.756043\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 87.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "beta = 0.0012\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    reg = tf.nn.l2_loss(weights)\n",
    "    new_loss = loss + beta*reg\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    #test_prediction_accuracy.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 0.00011, 0.00012, 0.00013000000000000002, 0.00014, 0.00015000000000000001, 0.00015999999999999999, 0.00017, 0.00017999999999999998, 0.00019, 0.00019999999999999998, 0.00021, 0.00021999999999999998, 0.00023, 0.00023999999999999998, 0.00025, 0.00026, 0.00027, 0.00028, 0.00029, 0.0003, 0.00031, 0.00031999999999999997, 0.00033, 0.00033999999999999997, 0.00035, 0.00035999999999999997, 0.00036999999999999994, 0.00037999999999999997, 0.00039, 0.00039999999999999996, 0.00040999999999999994, 0.00041999999999999996, 0.00043, 0.00043999999999999996, 0.00044999999999999993, 0.00045999999999999996, 0.00047, 0.00047999999999999996, 0.00049, 0.0005, 0.00051, 0.00052, 0.00053, 0.00054, 0.00055, 0.00056, 0.00057, 0.00058, 0.00059, 0.0006000000000000001, 0.00061, 0.00062, 0.00063, 0.0006399999999999999, 0.00065, 0.00066, 0.00067, 0.00068, 0.00069, 0.0007, 0.00071, 0.0007199999999999999, 0.00073, 0.00074, 0.00075, 0.00076, 0.00077, 0.00078, 0.00079, 0.0007999999999999999, 0.00081, 0.00082, 0.00083, 0.00084, 0.00085, 0.00086, 0.00087, 0.0008799999999999999, 0.00089, 0.0009, 0.00091, 0.00092, 0.0009299999999999999, 0.00094, 0.00095, 0.0009599999999999999, 0.0009699999999999999, 0.00098, 0.00099]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "#beta = np.arange(1e-4, 1e-3, 1e-5).tolist()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  reg = tf.nn.l2_loss(weights)\n",
    "  new_loss = loss + beta*reg\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.239731\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 35.2%\n",
      "Minibatch loss at step 500: 33.095695\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 65.6%\n",
      "Minibatch loss at step 1000: 32.817204\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 1500: 81.062981\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 59.4%\n",
      "Minibatch loss at step 2000: 106.502563\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 2500: 41.086510\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 3000: 27.881639\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 61.5%\n",
      "Test accuracy: 66.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
