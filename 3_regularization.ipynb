{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning with the logistic regression model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prediction_accuracy = []\n",
    "reg_par = np.arange(0.0005, 0.004, 0.0001)\n",
    "for i, beta in enumerate(reg_par):\n",
    "    \n",
    "  batch_size = 128\n",
    "\n",
    "  graph = tf.Graph()\n",
    "  with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    reg = tf.nn.l2_loss(weights)\n",
    "    new_loss = loss + beta*reg\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "  num_steps = 3001\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    #print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      #if (step % 500 == 0):\n",
    "        #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        #print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          #valid_prediction.eval(), valid_labels))\n",
    "    #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    test_prediction_accuracy.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maxiumum test accuracy was 87.81 % and was achieved with beta = 0.0016\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f40d4795250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(reg_par,test_prediction_accuracy)\n",
    "beta = reg_par[test_prediction_accuracy.index(max(test_prediction_accuracy))]\n",
    "print(\"The maxiumum test accuracy was\", max(test_prediction_accuracy), \"% and was achieved with beta =\", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the linear model with beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 14.946169\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 16.7%\n",
      "Minibatch loss at step 500: 1.137370\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 1000: 0.759638\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1500: 0.852644\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2000: 0.720361\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 2500: 0.469730\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 3000: 0.764444\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 87.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    reg = tf.nn.l2_loss(weights)\n",
    "    new_loss = loss + beta*reg\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation and test accuracies achieved here are higher the accuracies seen in the model without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add regularization to the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1e-05\n",
      "2e-05\n",
      "3e-05\n",
      "4e-05\n",
      "5e-05\n",
      "6e-05\n",
      "7e-05\n",
      "8e-05\n",
      "9e-05\n",
      "0.0001\n",
      "0.00011\n"
     ]
    }
   ],
   "source": [
    "test_prediction_accuracy = []\n",
    "reg_par = np.arange(0, 0.00012, 0.00001)\n",
    "for i, beta in enumerate(reg_par):\n",
    "\n",
    "    batch_size = 128\n",
    "    hidden_nodes = 1024\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, image_size * image_size))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "      # Variables.\n",
    "      weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "      biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "      weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "      biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "      # Training computation.\n",
    "      print(beta)\n",
    "      hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "      logits = tf.matmul(hidden_layer, weights2) + biases2\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "      reg = tf.nn.l2_loss(weights2)\n",
    "      new_loss = loss + beta*reg\n",
    "\n",
    "      # Optimizer.\n",
    "      optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      hidden_valid_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "      valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(hidden_valid_layer, weights2) + biases2)\n",
    "      hidden_test_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "      test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(hidden_test_layer, weights2) + biases2)\n",
    "\n",
    "    num_steps = 3001\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        #print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "          # Pick an offset within the training data, which has been randomized.\n",
    "          # Note: we could use better randomization across epochs.\n",
    "          offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "          # Generate a minibatch.\n",
    "          batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "          batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "          # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "          # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "          # and the value is the numpy array to feed to it.\n",
    "          feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "          _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "          #if (step % 500 == 0):\n",
    "            #print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            #print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            #print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "              #valid_prediction.eval(), valid_labels))\n",
    "        #print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "        test_prediction_accuracy.append(accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maxiumum test accuracy was 89.33 % and was achieved with beta = 8e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEACAYAAABCl1qQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1652981a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(reg_par,test_prediction_accuracy)\n",
    "beta = reg_par[test_prediction_accuracy.index(max(test_prediction_accuracy))]\n",
    "print(\"The maxiumum test accuracy was\", max(test_prediction_accuracy), \"% and was achieved with beta =\", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the neural network with the new beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8e-05\n",
      "Initialized\n",
      "Minibatch loss at step 0: 392.603027\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 31.9%\n",
      "Minibatch loss at step 500: 16.437206\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1000: 12.212330\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 7.242187\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 2000: 11.418255\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2500: 4.194616\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 3000: 2.914857\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "print(beta)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(hidden_layer, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  reg = tf.nn.l2_loss(weights2)\n",
    "  new_loss = loss + beta*reg\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  hidden_valid_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_valid_layer, weights2) + biases2)\n",
    "  hidden_test_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_test_layer, weights2) + biases2)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation and test accuracies here are higher than the ones achieved without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 361.740234\n",
      "Minibatch accuracy: 12.0%\n",
      "Validation accuracy: 32.2%\n",
      "Minibatch loss at step 500: 35.391491\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1000: 23.429144\n",
      "Minibatch accuracy: 86.0%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 1500: 10.590626\n",
      "Minibatch accuracy: 72.0%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 2000: 18.853075\n",
      "Minibatch accuracy: 66.0%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 2500: 2.251117\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 3000: 3.354944\n",
      "Minibatch accuracy: 72.0%\n",
      "Validation accuracy: 76.6%\n",
      "Test accuracy: 82.8%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(hidden_layer, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  reg = tf.nn.l2_loss(weights2)\n",
    "  new_loss = loss + beta*reg\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  hidden_valid_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_valid_layer, weights2) + biases2)\n",
    "  hidden_test_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_test_layer, weights2) + biases2)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy has fallen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing dropout for the non-overfitting case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8e-05\n",
      "Initialized\n",
      "Minibatch loss at step 0: 494.160309\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 34.5%\n",
      "Minibatch loss at step 500: 25.324768\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 19.668694\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 1500: 11.170761\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 2000: 5.513057\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2500: 6.642831\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 3000: 6.099822\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.6%\n",
      "Test accuracy: 87.6%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  hidden_layer = tf.nn.dropout(hidden_layer, keep_prob = 0.5)\n",
    "  logits = tf.matmul(hidden_layer, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  reg = tf.nn.l2_loss(weights2)\n",
    "  new_loss = loss + beta*reg\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  hidden_valid_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_valid_layer, weights2) + biases2)\n",
    "  hidden_test_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_test_layer, weights2) + biases2)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing dropout for the overfitting case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 460.918915\n",
      "Minibatch accuracy: 8.0%\n",
      "Validation accuracy: 28.7%\n",
      "Minibatch loss at step 500: 190.956177\n",
      "Minibatch accuracy: 72.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1000: 309.686920\n",
      "Minibatch accuracy: 74.0%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 1500: 128.150955\n",
      "Minibatch accuracy: 66.0%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 2000: 370.517822\n",
      "Minibatch accuracy: 58.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 2500: 20.480024\n",
      "Minibatch accuracy: 74.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 3000: 24.573271\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 68.6%\n",
      "Test accuracy: 74.3%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  hidden_layer = tf.nn.dropout(hidden_layer, keep_prob = 0.5)\n",
    "  logits = tf.matmul(hidden_layer, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  reg = tf.nn.l2_loss(weights2)\n",
    "  new_loss = loss + beta*reg\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(new_loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  hidden_valid_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_valid_layer, weights2) + biases2)\n",
    "  hidden_test_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_test_layer, weights2) + biases2)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 492.219513\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 41.5%\n",
      "Minibatch loss at step 500: 23.315767\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1000: 11.620200\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 11.867019\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2000: 5.390252\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2500: 5.117272\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 3000: 2.755826\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 83.0%\n",
      "Test accuracy: 89.3%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  hidden_layer = tf.nn.dropout(hidden_layer, keep_prob = 0.5)\n",
    "  logits = tf.matmul(hidden_layer, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  reg = tf.nn.l2_loss(weights2)\n",
    "  new_loss = loss + beta*reg\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.5)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  hidden_valid_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_valid_layer, weights2) + biases2)\n",
    "  hidden_test_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_test_layer, weights2) + biases2)\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
